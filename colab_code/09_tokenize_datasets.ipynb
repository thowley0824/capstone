{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"machine_shape":"hm","gpuType":"A100","authorship_tag":"ABX9TyM6lWkDE9c8ybwYnMJC21cP"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"0tKDKZabvf4G"},"outputs":[],"source":["!pip install datasets\n","!pip install transformers\n","\n","\n","import numpy as np\n","import pandas as pd\n","\n","from datasets import (Dataset, DatasetDict, ClassLabel,\n","                      concatenate_datasets, load_dataset)\n","from transformers import AutoTokenizer\n","\n","from huggingface_hub import notebook_login\n","notebook_login()"]},{"cell_type":"markdown","source":["**Define a helper functions for use in tokenizing the cleaned and preprocessed 8K text entries in the dataset_stacked_{2/3}_labels dataframes created in 08_create_and_push_datasets:**\n","\n","        def shape_tokenize_function(example):\n","\n","        def base_tokenize_function(examples):\n","\n","* **Functions input parameter is:**\n","         # The desired stacked dataframe containing event_id, labels, text, and\n","         #   the additional descriptor details added last script\n","         examples\n","\n","* **Function returns:**\n","        # The AutoTokenizer.from_pretrained({relevant_model_loc}) object that\n","        #     the name of the function indicates (sec-bert-shape for the #.\n","        #     shape_tokenize_function, sec-bert-base for the\n","        #     base_tokenize_function)\n","        base_tokenizer / shape_tokenizer object\n","\n","**These functions are soon aaplied to the stacked datasets via the dataset map functionality to efficiently create the tokenized data for both versions of the text input included as columns\n","\n","\n","* **Function steps through the following sequence:**\n","    * Creates a dictionary **event_regression_dict** containing a single entry:\n","            event_regression_dict['event_id'] = id\n","    * Creates a dataframe **event_est_win_data** from **estimation_window_df** containing only the data where the event_id = id\n","            estimation_window_df[estimation_window_df['event_id'] == event].copy()\n","    * Create vector **X** equal to the dataframe's market return column and add a constant\n","            X = event_est_win_data[['mkt_return']]\n","            X = sm.add_constant(X)\n","    * Create vector **y** from the dataframe's security return column\n","            y = event_est_win_data[['sec_return']]\n","\n","    * Use the stats_models OLS function to create **mod**, an object equal to the OLS regression model object using X and y\n","    * Create **est**, an object containing the results of the fit regression model\n","            mod = sm.OLS(y,X)\n","            est = mod.fit()\n","    * Using the attributes of the fitted model object **est**, add the following values to the **event_regression_dict**:\n","\n","            # The intercept of the fitted OLS model\n","            event_regression_dict['alpha'] = est.params['const']\n","            \n","            # The slope coefficient of the fitted OLS model\n","            event_regression_dict['beta'] = est.params['mkt_return']\n","\n","            # The standard error of the fitted model's residuals\n","            event_regression_dict['resid_std_error']** = np.sqrt(est.mse_resid)\n","\n"],"metadata":{"id":"094tPhoKnspG"}},{"cell_type":"code","source":["shape_tokenizer = AutoTokenizer.from_pretrained(\"nlpaueb/sec-bert-shape\")\n","base_tokenizer = AutoTokenizer.from_pretrained(\"nlpaueb/sec-bert-base\")\n","\n","def shape_tokenize_function(example):\n","    return shape_tokenizer(example[\"text_8k_sec_bert_shape\"],\n","                           padding = \"max_length\",\n","                           truncation=True)\n","\n","\n","def base_tokenize_function(examples):\n","    return base_tokenizer(examples['text_8k_sec_bert_base'],\n","                          padding = 'max_length',\n","                          truncation=True)"],"metadata":{"id":"YHeotVK3yl3R"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["dataset_2_labels = load_dataset(\"thowley824/dataset_stacked_2_labels\")\n","dataset_3_labels = load_dataset(\"thowley824/dataset_stacked_3_labels\")"],"metadata":{"id":"VWcmpR1JxET8"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Create several datasets from subsets of the overall 2 label and 3 label stacked datasets with different characteristics.**\n","\n","**NOTE:** for each subset we reference in the descriptions provided below, the subset should be assumed to have been generated for both the 2 label and the 3 label data\n","* This is assumed because it is impossible not to consider this data seperately, as the labels cause each of the respective datasets to have a fundamental difference in the definition of each's class label.\n","\n","\n","We will fine tune several of the eventual final resultant datasets that are created in this process.\n","\n","* They will allow us to observe whether there are any obvious patterns or differences between fine tuning performance based on the contrasting characteristics of the datasets (or conversely if the result is inconclusive).\n","\n","* Datasets with different combinations of the following differing characteristics will be generated in this process and pushed to the hub:\n","\n","    * **Text tokenized using sec-bert-base vs. text tokenized using sec-bert-shape** (these cannot remain in the same Dataset because the model accepts specifically formatted Datasets as training and evaluation data)\n","    * Data with **labels representing 2 bins** vs. data with **labels representing 3 bins**\n","    * Labels based on the **non-standardized CAR values** associated with events vs. labels based on the **standardized CAR values** associated with the events**\n","    * Labels generated from the results of event studies with a **long event window (starting 5 days before the event date, ending 5 days after)** vs. labels generated from the results of event studies with a **short event window (starting 1 day before the event, ending 1 day after)**\n","        * **NOTE:** in the prior work, we have generated event study results based on many different event windows.\n","            * However, due to time constraints, rather than testing all possible permutations (of which there are 72) we will only fine-tune the data corresponding to the longest and shortest symetric windows we utilized\n"],"metadata":{"id":"Q9X0cnNpq9Jn"}},{"cell_type":"markdown","source":["**Step 1: Create long and short window datasets from the overall stacked data**\n","\n","* To do so, pass a lambda function to the Dataset filter function resulting in the retention of only data with the desired event window start and ends."],"metadata":{"id":"JVUpF5zav_6Z"}},{"cell_type":"code","source":["long_window_2_labels = dataset_2_labels.filter(lambda x: (x['event_window_start']==-5)&(x['event_window_end']==5))\n","long_window_3_labels = dataset_3_labels.filter(lambda x: (x['event_window_start']==-5)&(x['event_window_end']==5))\n","\n","short_window_2_labels = dataset_2_labels.filter(lambda x: (x['event_window_start']==-1)&(x['event_window_end']==1))\n","short_window_3_labels = dataset_3_labels.filter(lambda x: (x['event_window_start']==-1)&(x['event_window_end']==1))"],"metadata":{"id":"zLUDOzcH28p7"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Step 2: Create shape tokenized and base tokenized datasets from the long and short window data created in Step 1.**\n","\n","* To do so, pass the corresponding tokenize function defined above to the Dataset map function."],"metadata":{"id":"rDDmo7_XxLt0"}},{"cell_type":"code","source":["shape_long_window_2_labels = long_window_2_labels.map(\n","    shape_tokenize_function, batched=True)\n","\n","shape_long_window_3_labels = long_window_3_labels.map(\n","    shape_tokenize_function, batched=True)\n","\n","base_long_window_2_labels = long_window_2_labels.map(\n","    base_tokenize_function, batched=True)\n","\n","base_long_window_3_labels = long_window_3_labels.map(\n","    base_tokenize_function, batched=True)\n","\n","shape_short_window_2_labels = short_window_2_labels.map(\n","    shape_tokenize_function, batched=True)\n","\n","shape_short_window_3_labels = short_window_3_labels.map(\n","    shape_tokenize_function, batched=True)\n","\n","base_short_window_2_labels = short_window_2_labels.map(\n","    base_tokenize_function, batched=True)\n","\n","base_short_window_3_labels = short_window_3_labels.map(\n","    base_tokenize_function, batched=True)"],"metadata":{"id":"C6zfRbJ13chg"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Step 3: Create CAR-label-based and SCAR-label-based datasets the long and short window / base and shape tokenized data created in Step 2.**\n","\n","**Step 4: Remove all columns from every dataset created in Step 3 except for those created by the tokenizer and the label column.**\n","\n","**Step 5: Rename the label column labels to conform with model input requirements.**\n","\n","* To do so, pass the corresponding tokenize function defined above to the Dataset map function."],"metadata":{"id":"m4ye_aF3x1-K"}},{"cell_type":"code","source":["remove_columns = [\n","    'event_id','text_8k_sec_bert_base','text_8k_sec_bert_shape',\n","    'event_window_start','event_window_end','abnormal_return_metric']\n","\n","shape_long_window_2_labels_car = shape_long_window_2_labels.filter(\n","    lambda x: (x['abnormal_return_metric']=='car'))\n","shape_long_window_2_labels_car = shape_long_window_2_labels_car.map(\n","    remove_columns = remove_columns)\n","shape_long_window_2_labels_car = shape_long_window_2_labels_car.rename_column(\n","    \"label\", \"labels\")\n","\n","shape_long_window_3_labels_car = shape_long_window_3_labels.filter(\n","    lambda x: (x['abnormal_return_metric']=='car'))\n","shape_long_window_3_labels_car = shape_long_window_3_labels_car.map(\n","    remove_columns = remove_columns)\n","shape_long_window_3_labels_car = shape_long_window_3_labels_car.rename_column(\n","    \"label\", \"labels\")\n","\n","shape_long_window_2_labels_scar = shape_long_window_2_labels.filter(\n","    lambda x: (x['abnormal_return_metric']=='scar'))\n","shape_long_window_2_labels_scar = shape_long_window_2_labels_scar.map(\n","    remove_columns = remove_columns)\n","shape_long_window_2_labels_scar = shape_long_window_2_labels_scar.rename_column(\n","    \"label\", \"labels\")\n","\n","shape_long_window_3_labels_scar = shape_long_window_3_labels.filter(\n","    lambda x: (x['abnormal_return_metric']=='scar'))\n","shape_long_window_3_labels_scar = shape_long_window_3_labels_scar.map(\n","    remove_columns = remove_columns)\n","shape_long_window_3_labels_scar = shape_long_window_3_labels_scar.rename_column(\n","    \"label\", \"labels\")\n","\n","base_long_window_2_labels_car = base_long_window_2_labels.filter(\n","    lambda x: (x['abnormal_return_metric']=='car'))\n","base_long_window_2_labels_car = base_long_window_2_labels_car.map(\n","    remove_columns = remove_columns)\n","base_long_window_2_labels_car = base_long_window_2_labels_car.rename_column(\n","    \"label\", \"labels\")\n","\n","base_long_window_2_labels_scar = base_long_window_2_labels.filter(\n","    lambda x: (x['abnormal_return_metric']=='scar'))\n","base_long_window_2_labels_scar = base_long_window_2_labels_scar.map(\n","    remove_columns = remove_columns)\n","base_long_window_2_labels_scar = base_long_window_2_labels_scar.rename_column(\n","    \"label\", \"labels\")\n","\n","base_long_window_3_labels_car = base_long_window_3_labels.filter(\n","    lambda x: (x['abnormal_return_metric']=='car'))\n","base_long_window_3_labels_car = base_long_window_3_labels_car.map(\n","    remove_columns = remove_columns)\n","base_long_window_3_labels_car = base_long_window_3_labels_car.rename_column(\n","    \"label\", \"labels\")\n","\n","base_long_window_3_labels_scar = base_long_window_3_labels.filter(\n","    lambda x: (x['abnormal_return_metric']=='scar'))\n","base_long_window_3_labels_scar = base_long_window_3_labels_scar.map(\n","    remove_columns = remove_columns)\n","base_long_window_3_labels_scar = base_long_window_3_labels_scar.rename_column(\n","    \"label\", \"labels\")\n","\n","shape_short_window_2_labels_car = shape_short_window_2_labels.filter(\n","    lambda x: (x['abnormal_return_metric']=='car'))\n","shape_short_window_2_labels_car = shape_short_window_2_labels_car.map(\n","    remove_columns = remove_columns)\n","shape_short_window_2_labels_car = shape_short_window_2_labels_car.rename_column(\n","    \"label\", \"labels\")\n","\n","shape_short_window_2_labels_scar = shape_short_window_2_labels.filter(\n","    lambda x: (x['abnormal_return_metric']=='scar'))\n","shape_short_window_2_labels_scar = shape_short_window_2_labels_scar.map(\n","    remove_columns = remove_columns)\n","shape_short_window_2_labels_scar = shape_short_window_2_labels_scar.rename_column(\n","    \"label\", \"labels\")\n","\n","shape_short_window_3_labels_car = shape_short_window_3_labels.filter(\n","    lambda x: (x['abnormal_return_metric']=='car'))\n","shape_short_window_3_labels_car = shape_short_window_3_labels_car.map(\n","    remove_columns = remove_columns)\n","shape_short_window_3_labels_car = shape_short_window_3_labels_car.rename_column(\n","    \"label\", \"labels\")\n","\n","shape_short_window_3_labels_scar = shape_short_window_3_labels.filter(\n","    lambda x: (x['abnormal_return_metric']=='scar'))\n","shape_short_window_3_labels_scar = shape_short_window_3_labels_scar.map(\n","    remove_columns = remove_columns)\n","shape_short_window_3_labels_scar = shape_short_window_3_labels_scar.rename_column(\n","    \"label\", \"labels\")\n","\n","base_short_window_2_labels_car = base_short_window_2_labels.filter(\n","    lambda x: (x['abnormal_return_metric']=='car'))\n","base_short_window_2_labels_car = base_short_window_2_labels_car.map(\n","    remove_columns = remove_columns)\n","base_short_window_2_labels_car = base_short_window_2_labels_car.rename_column(\n","    \"label\", \"labels\")\n","\n","base_short_window_2_labels_scar = base_short_window_2_labels.filter(\n","    lambda x: (x['abnormal_return_metric']=='scar'))\n","base_short_window_2_labels_scar = base_short_window_2_labels_scar.map(\n","    remove_columns = remove_columns)\n","base_short_window_2_labels_scar = base_short_window_2_labels_scar.rename_column(\n","    \"label\", \"labels\")\n","\n","base_short_window_3_labels_car = base_short_window_3_labels.filter(\n","    lambda x: (x['abnormal_return_metric']=='car'))\n","base_short_window_3_labels_car = base_short_window_3_labels_car.map(\n","    remove_columns = remove_columns)\n","base_short_window_3_labels_car = base_short_window_3_labels_car.rename_column(\n","    \"label\", \"labels\")\n","\n","base_short_window_3_labels_scar = base_short_window_3_labels.filter(\n","    lambda x: (x['abnormal_return_metric']=='scar'))\n","base_short_window_3_labels_scar = base_short_window_3_labels_scar.map(\n","    remove_columns = remove_columns)\n","base_short_window_3_labels_scar = base_short_window_3_labels_scar.rename_column(\n","    \"label\", \"labels\")"],"metadata":{"id":"mFuXuCYdBwDK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["shape_long_window_2_labels_car.push_to_hub('shape_long_window_2_labels_car')\n","shape_long_window_3_labels_car.push_to_hub('shape_long_window_3_labels_car')\n","base_long_window_2_labels_car.push_to_hub('base_long_window_2_labels_car')\n","base_long_window_3_labels_car.push_to_hub('base_long_window_3_labels_car')\n","shape_short_window_2_labels_car.push_to_hub('shape_short_window_2_labels_car')\n","shape_short_window_3_labels_car.push_to_hub('shape_short_window_3_labels_car')\n","base_short_window_2_labels_car.push_to_hub('base_short_window_2_labels_car')\n","base_short_window_3_labels_car.push_to_hub('base_short_window_3_labels_car')\n","\n","shape_long_window_2_labels_scar.push_to_hub('shape_long_window_2_labels_scar')\n","shape_long_window_3_labels_scar.push_to_hub('shape_long_window_3_labels_scar')\n","base_long_window_2_labels_scar.push_to_hub('base_long_window_2_labels_scar')\n","base_long_window_3_labels_scar.push_to_hub('base_long_window_3_labels_scar')\n","shape_short_window_2_labels_scar.push_to_hub('shape_short_window_2_labels_scar')\n","shape_short_window_3_labels_scar.push_to_hub('shape_short_window_3_labels_scar')\n","base_short_window_2_labels_scar.push_to_hub('base_short_window_2_labels_scar')\n","base_short_window_3_labels_scar.push_to_hub('base_short_window_3_labels_scar')"],"metadata":{"id":"y1xYwfh6dLPQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["tokenized_datasets.set_format(\"torch\")\n","\n","train_dataset = tokenized_datasets[\"train\"]\n","eval_dataset = tokenized_datasets[\"test\"]\n","\n","from torch.utils.data import DataLoader\n","\n","train_dataloader = DataLoader(train_dataset, shuffle=True, batch_size=8)\n","eval_dataloader = DataLoader(eval_dataset, batch_size=8)"],"metadata":{"id":"0wMsKkfeQQ1F"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from accelerate import Accelerator\n","from transformers import AdamW, AutoModelForSequenceClassification, get_scheduler\n","\n","accelerator = Accelerator()\n","\n","model = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=2)\n","optimizer = AdamW(model.parameters(), lr=3e-5)\n","\n","train_dataloader, eval_dataloader, model, optimizer = accelerator.prepare(\n","    train_dataloader, eval_dataloader, model, optimizer)\n","\n","  num_epochs = 3\n","  num_training_steps = num_epochs * len(train_dataloader)\n","  lr_scheduler = get_scheduler(\n","      \"linear\",\n","      optimizer=optimizer,\n","      num_warmup_steps=0,\n","      num_training_steps=num_training_steps\n","  )\n","\n","  progress_bar = tqdm(range(num_training_steps))\n","\n","  model.train()\n","  for epoch in range(num_epochs):\n","      for batch in train_dataloader:\n","-         batch = {k: v.to(device) for k, v in batch.items()}\n","          outputs = model(**batch)\n","          loss = outputs.loss\n","-         loss.backward()\n","+         accelerator.backward(loss)\n","\n","          optimizer.step()\n","          lr_scheduler.step()\n","          optimizer.zero_grad()\n","          progress_bar.update(1)"],"metadata":{"id":"itbJPIeBPN1G"},"execution_count":null,"outputs":[]}]}