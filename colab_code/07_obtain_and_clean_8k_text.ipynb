{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"machine_shape":"hm","gpuType":"L4","authorship_tag":"ABX9TyMJQvQZIj7ZkYKUM0APfC/N"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"sRI4rYVRt5RE"},"outputs":[],"source":["!wget https://raw.githubusercontent.com/thowley0824/capstone/main/colab_initialization/initializer.py\n","!pip install --no-dependencies wrds\n","\n","import numpy as np\n","import pandas as pd\n","import os\n","import requests\n","import re\n","import string\n","import pickle\n","from bs4 import BeautifulSoup as bs\n","\n","import initializer\n","initializer.initialize_colab()"]},{"cell_type":"code","source":["'''\n","SET PROCESSED DATA SUBDIRECTORIES AND FORM TYPE PREFIX\n","WHEN APPLICABLE, THIS FORM TYPE PREFIX WILL BE USED MOVING FORWARD\n","'''\n","\n","linking_data_subdir = 'data/edgar_wrds_linking/'\n","q_cleaned_text_data_subdir = 'data/sec_edgar/8k_text_cleaned_quarterly/'\n","sec_edgar_data_subdir = 'data/sec_edgar/'\n","file_prefix = '8k_'\n","\n","'''\n","FILE NAMES CARRIED DOWN FROM PRIOR WORK\n","'''\n","\n","event_subset_file_name = 'event_subset.pkl'\n","master_index_all_periods_file_name = 'master_index_all_periods.pkl'\n","\n","'''\n","NEW FILE NAMES FOR USE BELOW\n","'''\n","\n","text_cleaned_file_name = 'text_cleaned.pkl'"],"metadata":{"id":"pVLJUnafw1Q3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["'''\n","READ IN EVENT SUBSET DATA\n","'''\n","\n","event_subset = pd.read_pickle(\n","    linking_data_subdir +\n","    file_prefix +\n","    event_subset_file_name)\n","\n","'''\n","READ IN ALL PERIOD MASTER INDEX DATA\n","'''\n","\n","master_index_all_periods = pd.read_pickle(\n","    linking_data_subdir +\n","    file_prefix +\n","    master_index_all_periods_file_name)"],"metadata":{"id":"xcwlj_uIxWXC"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**HELPER FUNCTION: for use obtaining each event's raw 8K HTML**\n","\n","        get_8k_text(webloc_8k)\n","\n","* **Function input parameters are:**\n","        # The web location of a given event's 8K document\n","        webloc_8k\n","\n","* **Function returns:**\n","        # The raw HTML version of the event's 8K document text\n","        r.text"],"metadata":{"id":"agH6VCTXx4hV"}},{"cell_type":"code","source":["header_content = {'User-Agent': 'Georgia Tech thowley3@georgiatech.edu',\n","                  'Host': 'www.sec.gov'}\n","\n","def get_8k_text(webloc_8k):\n","\n","    request_args = {'url': webloc_8k,\n","                    'headers': header_content}\n","    r = requests.get(**request_args)\n","    return r.text"],"metadata":{"id":"_615d9fux4r3"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**HELPER FUNCTION: for use in cleaning each event's raw 8K HTML**\n","\n","* **NOTE: Code leverages the text cleaning process defined in the NLP Code Tutorial Part 1 provided in class resources***\n","\n","        clean_8k_text(text)\n","\n","* **Function input parameters are:**\n","        # The raw HTML output obtained from the SEC EDGAR data related to an individual event (8K)\n","        input_text\n","\n","* **Function returns:**\n","        # The string version of the section of the event's 8K text relevant for use in LLM classification\n","        text"],"metadata":{"id":"HNHZhJ3zuoyR"}},{"cell_type":"code","source":["def clean_8k_text(input_text):\n","\n","    input_text = re.sub(r'(\\r\\n|\\r|\\n)',' ', input_text)\n","\n","    # remove certain text with regex query\n","    input_text = re.sub(\n","        r'<DOCUMENT>\\s*<TYPE>(?:GRAPHIC|ZIP|EXCEL|PDF|XML|JSON).*?</DOCUMENT>',\n","        ' ', input_text)\n","    input_text = re.sub(r'<SEC-HEADER>.*?</SEC-HEADER>',' ', input_text)\n","    input_text = re.sub(r'<IMS-HEADER>.*?</IMS-HEADER>',\n","                        ' ', input_text)\n","\n","    # replace characters to correct them\n","    input_text = re.sub(r'&nbsp;', ' ', input_text)\n","    input_text = re.sub(r'&#160;', ' ', input_text)\n","    input_text = re.sub(r'&amp;', '&', input_text)\n","    input_text = re.sub(r'&#38;','&', input_text)\n","\n","    # replace other encoded characters to whitespace\n","    input_text = re.sub(r'&([a-z0-9]+|#[0-9]{1,6}|#x[0-9a-f]{1,6});',\n","                        ' ', input_text)\n","\n","    soup = bs(input_text, 'html.parser')\n","\n","    for tag in soup.find_all('xbrl'):\n","        # don't remove if there is item detail\n","        fp_result = tag(text=re.compile(r'(?i)item\\s*\\d', re.IGNORECASE))\n","        event = len(fp_result)\n","\n","    ## if no item details remove that part\n","    # decompose() method removes a tag as well as its inner content.\n","        if (event==0):\n","            tag.decompose()\n","\n","    # remove tables\n","    for tag in soup.find_all('table'):\n","        temp_text = tag.get_text()\n","        numbers = sum(c.isdigit() for c in temp_text)\n","        letters = sum(c.isalpha() for c in temp_text)\n","        ratio_number_letter = 1.0\n","\n","        if (numbers + letters) > 0:\n","            ratio_number_letter = numbers/(numbers + letters)\n","\n","        event = 0\n","\n","        if( (event==0) and ( ratio_number_letter > 0.1)):\n","            tag.decompose()\n","\n","    ## remove other text between tags used for styling\n","    text = soup.get_text()\n","    text = re.sub(r'<(?:ix|link|xbrli|xbrldi).*?>.*?<\\/.*?>', ' ', text)\n","\n","    ## remove extra whitespace from sentences\n","    text = \"\".join(line.strip() for line in text.split(\"\\n\"))\n","\n","    ## some additional cleaning\n","    text = re.sub(r'--;', ' ', text)\n","    text = re.sub(r'__', ' ', text)\n","\n","    cleanr = re.compile(r'<.*?>|&([a-z0-9]+|#[0-9]{1,6}|#x[0-9a-f]{1,6});')\n","    text = re.sub(cleanr, ' ', text)\n","\n","    temp_match = re.search(r'^.*?item(\\s)*\\d', text, flags=re.IGNORECASE)\n","    if temp_match != None:\n","        text = re.sub(r'^.*?item(\\s)*\\d', '', text, count=1,\n","                      flags=re.IGNORECASE)\n","\n","    ## replace more than one whitespace with single whitespace\n","    text = re.sub(r'\\s+', ' ', text)\n","    return text"],"metadata":{"id":"pt3JdaD3uJSS"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":[" **Step 1:**\n","\n","* **Create A List Containing a Sample of Record Dicts From The Event Subset Data**\n","    * For each quarterly period in the event subset, generate a sample of 500 events for which we will obtain 8K data\n","    * Create a list containing each period's sample\n","    * Will allow us to write the output in pieces for each period to monitor the text obtention process and manage any errors (disconnecting, etc.)\n","    * First period is 2005 Q1, last period is 2018 Q4\n","    * 56 periods; 28,000 events\n","    * Each record dict is a list of dictionaries, with each individual dictionary corresponding to an individual event\n","    * Each dictionary key is the event ID we created when we initially created the all-period master index dataset\n","    * Each dictionary value is the web location corresponding to that event ID's 8k doc for the event"],"metadata":{"id":"pgAlaqvJyotD"}},{"cell_type":"code","source":["sec_html_prefix = 'https://www.sec.gov/Archives/'\n","\n","event_subset = event_subset[['event_id','period']]\n","event_subset = event_subset.groupby('period').sample(n=500, random_state=1\n","                                                ).reset_index(drop = True)\n","\n","master_index_all_periods['webloc'] = (\n","    sec_html_prefix + master_index_all_periods['filename'])\n","\n","master_index_all_periods = master_index_all_periods[['event_id','webloc']]\n","\n","event_subset = event_subset.merge(master_index_all_periods, on='event_id')\n","\n","n = 500\n","event_subset_lst = [event_subset[i:i+n].to_dict('records') for i\n","                    in range(0,len(event_subset),n)]"],"metadata":{"id":"FtIkaIyVuJfC"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":[" **Step 2:**\n","\n","* **Obtain, Clean, and Write 8K Text For Each Event**\n","    * For each period in the event subset:\n","        * Set the name of the output location for the event-8K text data using the name of the current period\n","        * Create an empty dictionary clean_text_output_dict for storing the cleaned text output for the current period\n","        * For each event in the period sample:\n","            * Get the raw 8K text using the get_8k_text helper function\n","            * Get the clean 8K text using the clean_8k_text helper function\n","            * Add the clean 8K text to the clean_text_output_dict\n","        * Write the period's clean_text_output_dict to the file location specified as a pickle file\n","\n","* **NOTE:**\n","    * A handful of the event's raw 8K text causes an error when attempting to clean the text due to the HTML being irregularly formatted\n","    * To handle this data, I've included a try-except clause that skips any data that raises an exception and does not include this data in the cleaned text output\n","    * Try-except clause simply skips this event and continues iteration\n","    * Insignificant number of documents cause this issue (20 / 28,000) so decision was made not to replace this data"],"metadata":{"id":"ucVS4FE20o_Q"}},{"cell_type":"code","source":["for i in range(1,len(event_subset_lst)):\n","\n","    file_name = f\"{event_subset_lst[i][0]['period']}.pkl\"\n","    output_loc = q_cleaned_text_data_subdir + file_name\n","\n","    clean_text_output_dict = {}\n","\n","    for event in event_subset_lst[i]:\n","\n","        text_8k = get_8k_text(event['webloc'])\n","        try:\n","            text_8k = clean_8k_text(text_8k)\n","        except:\n","            print(f\"\"\"\n","Error with index: {event['event_id']}, webloc: {event['webloc']}\"\"\")\n","            continue\n","        clean_text_output_dict[event['event_id']] = text_8k\n","\n","\n","    with open(output_loc, 'wb') as f:\n","        pickle.dump(clean_text_output_dict, f)\n","\n","    print(f\"\"\"\n","Cleaned 8k text output generated and written to {output_loc}.\"\"\")"],"metadata":{"id":"OG5MewimuJhz"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":[" **Step 3:**\n","\n","* **Create a Combined Cleaned 8K Text Dataframe From The Cleaned 8K Text Data**\n","    * Create an empty list for holding each individual 8k text dataframe\n","    * Read in the event_id - 8k text dictionary created for each period above\n","    * Create a dataframe containing the event_id - 8K text data from each dict\n","    * Add each dataframe to the list, and create a single combined dataframe from the list of dataframes above\n","        * Set the name of the output location for the event-8K text data using the name of the current period\n","        * Create an empty dictionary clean_text_output_dict for storing the cleaned text output for the current period\n","        * For each event in the period sample:\n","            * Get the raw 8K text using the get_8k_text helper function\n","            * Get the clean 8K text using the clean_8k_text helper function\n","            * Add the clean 8K text to the clean_text_output_dict\n","        * Write the period's clean_text_output_dict to the file location specified as a pickle file"],"metadata":{"id":"AZo4T71KHi14"}},{"cell_type":"code","source":["combined_event_8k_text_lst = []\n","\n","for i in range(len(event_subset_lst)):\n","    file_name = f\"{event_subset_lst[i][0]['period']}.pkl\"\n","    input_loc = q_cleaned_text_data_subdir + file_name\n","\n","    with open(input_loc,'rb') as f:\n","        text = pickle.load(f)\n","\n","    text_df = pd.DataFrame.from_dict(text,\n","                                    orient='index',\n","                                    columns = ['text_8k']\n","                                    ).reset_index(names = ['event_id'])\n","\n","    combined_event_8k_text_lst.append(text_df)\n","\n","combined_cleaned_8k_text = pd.concat(combined_event_8k_text_lst\n","                                    ).sort_values(by = ['event_id']\n","                                                 ).reset_index(\n","                                                     drop = True)"],"metadata":{"id":"6rK5gdjxuJkF"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["combined_cleaned_8k_text.to_pickle((\n","    sec_edgar_data_subdir +\n","    file_prefix +\n","    text_cleaned_file_name\n","    ))"],"metadata":{"id":"vx6kL9yGI3CJ"},"execution_count":null,"outputs":[]}]}