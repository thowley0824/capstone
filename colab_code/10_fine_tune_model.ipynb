{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"UYJvsPRfd-ho"},"outputs":[],"source":["%%capture\n","!wget https://raw.githubusercontent.com/thowley0824/capstone/main/colab_initialization/initializer.py\n","!pip install --no-dependencies wrds\n","\n","!pip install datasets\n","!pip install transformers\n","!pip install evaluate\n","!pip install accelerate\n","\n","import evaluate\n","import numpy as np\n","import os\n","import pandas as pd\n","import pickle\n","import torch\n","\n","from accelerate import (Accelerator,\n","                        notebook_launcher)\n","from datasets import (Dataset, DatasetDict, ClassLabel,\n","                      concatenate_datasets, load_dataset)\n","from torch.optim import AdamW\n","from torch.utils.data import DataLoader\n","from tqdm.auto import tqdm\n","from transformers import get_scheduler\n","from transformers import (AutoModelForSequenceClassification,\n","                          AutoTokenizer,\n","                          get_scheduler)\n","\n","import initializer\n","from huggingface_hub import notebook_login"]},{"cell_type":"markdown","source":["**THE CELL BELOW ONLY NEEDS TO BE RUN ONE TIME**\n","* Google Colab had no accelerate config file defined\n","* This makes it impossible to leverage accelerate functionality\n","to speedup model fine-tuning tasks\n","* The `write_basic_config` function call generates a basic default config file for non-TPU, non-multi GPU accelerate usage (I am running my fine-tuning on a single A100 GPU, so this is sufficient for the work here)\n","* After writing this config, the notebook must be exited and restarted in order to source this config file for use.\n","* Following this step, the config file will remain instantiated, and will not require future calls.  "],"metadata":{"id":"vZi6AZA36vu0"}},{"cell_type":"code","source":["'''\n","COMMENTING OUT THE ACCELERATE write_basic_config FUNCTION CALL\n","TO AVOID UNNNEEDED RUNS IN THE FUTURE\n","'''\n","\n","'''\n","from accelerate.utils import write_basic_config\n","\n","write_basic_config()  # Write a config file\n","os._exit(00)  # Re\n","'''"],"metadata":{"id":"uBMIZXBW6sNT"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["initializer.initialize_colab()\n","notebook_login()"],"metadata":{"id":"dLFgKhpsw47H"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["fine_tuning_result_loc = ('data/fine_tuning_result/'+\n","                          'fine_tuning_result.pkl')\n","\n","if os.path.exists(fine_tuning_result_loc):\n","\n","    with open(fine_tuning_result_loc, 'rb') as f:\n","        model_results_record = pickle.load(f)\n","else:\n","    model_results_record = []\n","\n","model_results_record"],"metadata":{"id":"CXedUCxQM1-p"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6z0VuO3perCy"},"outputs":[],"source":["def training_function(model, dataset_name):\n","\n","    accelerator = Accelerator()\n","\n","    train_dataloader = DataLoader(train_dataset, shuffle=True, batch_size=8)\n","    eval_dataloader = DataLoader(eval_dataset, batch_size=32)\n","\n","    optimizer = AdamW(model.parameters(), lr=3e-5)\n","\n","    train_dataloader, eval_dataloader, model, optimizer = accelerator.prepare(\n","     train_dataloader, eval_dataloader, model, optimizer)\n","\n","    num_epochs = 3\n","    num_training_steps = num_epochs * len(train_dataloader)\n","    lr_scheduler = get_scheduler(\"linear\",\n","        optimizer=optimizer,\n","        num_warmup_steps=0,\n","        num_training_steps=num_training_steps)\n","\n","    progress_bar = tqdm(range(num_training_steps))\n","\n","    model.train()\n","    for epoch in range(num_epochs):\n","        for batch in train_dataloader:\n","            outputs = model(**batch)\n","            loss = outputs.loss\n","            accelerator.backward(loss)\n","\n","            optimizer.step()\n","            lr_scheduler.step()\n","            optimizer.zero_grad()\n","            progress_bar.update(1)\n","\n","    metric=evaluate.combine([\"accuracy\", \"recall\", \"precision\", \"f1\"])\n","\n","    model.eval()\n","    for batch in eval_dataloader:\n","        with torch.no_grad():\n","            outputs = model(**batch)\n","\n","        logits = outputs.logits\n","        predictions = torch.argmax(logits, dim=-1)\n","\n","        metric.add_batch(predictions=predictions,\n","                         references=batch[\"labels\"],\n","                         average='weighted')\n","\n","\n","    eval_metric = metric.compute()\n","    eval_metric['dataset_name'] = dataset_name\n","\n","    model_results_record.append(eval_metric)\n","\n","    with open(fine_tuning_result_loc, 'wb') as f:\n","        pickle.dump(model_results_record, f)\n","\n","    accelerator.print(eval_metric)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"K5hDywKTp2cd"},"outputs":[],"source":["model_1 = AutoModelForSequenceClassification.from_pretrained(\n","        \"nlpaueb/sec-bert-shape\", num_labels=2)\n","\n","curr_dataset_name = 'shape_long_window_2_labels_scar'\n","\n","curr_dataset = load_dataset(f\"\"\"thowley824/{curr_dataset_name}\"\"\")\n","curr_dataset.set_format(\"torch\")\n","\n","train_dataset = curr_dataset[\"train\"]\n","eval_dataset = curr_dataset[\"test\"]\n","\n","print(f\"\"\"FINE-TUNING DATASET AND MODEL SUMMARY:\n","Model:                  nlpaueb/sec-bert-shape\n","Number of Labels:       2\n","Abnormal Return Metric: Standardized CAR\n","Event Window:           5 days before event through 5 days after event\"\"\")\n","\n","eval_result = notebook_launcher(training_function, (model_1,\n","                                                    curr_dataset_name))"]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"id":"XO_UICsH5d9i"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model_2 = AutoModelForSequenceClassification.from_pretrained(\n","        \"nlpaueb/sec-bert-base\", num_labels=2)\n","\n","curr_dataset_name = 'base_long_window_2_labels_scar'\n","\n","curr_dataset = load_dataset(f\"thowley824/{curr_dataset_name}\")\n","\n","curr_dataset.set_format(\"torch\")\n","\n","train_dataset = curr_dataset[\"train\"]\n","eval_dataset = curr_dataset[\"test\"]\n","\n","print(f\"\"\"FINE-TUNING DATASET AND MODEL SUMMARY:\n","Model:                  nlpaueb/sec-bert-base\n","Number of Labels:       2\n","Abnormal Return Metric: Standardized CAR\n","Event Window:           5 days before event through 5 days after event\"\"\")\n","\n","notebook_launcher(training_function, (model_2,\n","                                      curr_dataset_name))"],"metadata":{"id":"FsNpxdslN2o9"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model_3 = AutoModelForSequenceClassification.from_pretrained(\n","        \"nlpaueb/sec-bert-shape\", num_labels=2)\n","\n","curr_dataset_name = 'shape_long_window_2_labels_car'\n","\n","curr_dataset = load_dataset(f\"thowley824/{curr_dataset_name}\")\n","curr_dataset.set_format(\"torch\")\n","\n","train_dataset = curr_dataset[\"train\"]\n","eval_dataset = curr_dataset[\"test\"]\n","\n","print(f\"\"\"FINE-TUNING DATASET AND MODEL SUMMARY:\n","Model:                  nlpaueb/sec-bert-shape\n","Number of Labels:       2\n","Abnormal Return Metric: CAR\n","Event Window:           5 days before event through 5 days after event\"\"\")\n","\n","notebook_launcher(training_function, (model_3,\n","                                      curr_dataset_name))"],"metadata":{"id":"sQeS-BbqN7gr"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"Z-Ry7BzKiv98"},"outputs":[],"source":["model_4 = AutoModelForSequenceClassification.from_pretrained(\n","        \"nlpaueb/sec-bert-base\", num_labels=2)\n","\n","curr_dataset_name = 'base_long_window_2_labels_car'\n","\n","curr_dataset = load_dataset(f\"thowley824/{curr_dataset_name}\")\n","curr_dataset.set_format(\"torch\")\n","\n","train_dataset = curr_dataset[\"train\"]\n","eval_dataset = curr_dataset[\"test\"]\n","\n","print(f\"\"\"FINE-TUNING DATASET AND MODEL SUMMARY:\n","Model:                  nlpaueb/sec-bert-base\n","Number of Labels:       2\n","Abnormal Return Metric: CAR\n","Event Window:           5 days before event through 5 days after event\"\"\")\n","\n","notebook_launcher(training_function, (model_4,))"]},{"cell_type":"code","source":["model_5 = AutoModelForSequenceClassification.from_pretrained(\n","        \"nlpaueb/sec-bert-shape\", num_labels=2)\n","\n","curr_dataset_name = 'shape_short_window_2_labels_scar'\n","\n","curr_dataset = load_dataset(f\"thowley824/{curr_dataset_name}\")\n","curr_dataset.set_format(\"torch\")\n","\n","train_dataset = curr_dataset[\"train\"]\n","eval_dataset = curr_dataset[\"test\"]\n","\n","print(f\"\"\"FINE-TUNING DATASET AND MODEL SUMMARY:\n","Model:                  nlpaueb/sec-bert-shape\n","Number of Labels:       2\n","Abnormal Return Metric: Standardized CAR\n","Event Window:           1 day before event through 1 day after event\"\"\")\n","\n","notebook_launcher(training_function, (model_5,\n","                                      curr_dataset_name))"],"metadata":{"id":"tjmTSAZEPKCB"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model_6 = AutoModelForSequenceClassification.from_pretrained(\n","        \"nlpaueb/sec-bert-base\", num_labels=2)\n","\n","curr_dataset_name = 'base_short_window_2_labels_scar'\n","\n","curr_dataset = load_dataset(f\"thowley824/{curr_dataset_name}\")\n","curr_dataset.set_format(\"torch\")\n","\n","train_dataset = curr_dataset[\"train\"]\n","eval_dataset = curr_dataset[\"test\"]\n","\n","print(f\"\"\"FINE-TUNING DATASET AND MODEL SUMMARY:\n","Model:                  nlpaueb/sec-bert-base\n","Number of Labels:       2\n","Abnormal Return Metric: Standardized CAR\n","Event Window:           1 day before event through 1 day after event\"\"\")\n","\n","notebook_launcher(training_function, (model_6,\n","                                      curr_dataset_name))"],"metadata":{"id":"C8WOSJAzPKJz"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model_7 = AutoModelForSequenceClassification.from_pretrained(\n","        \"nlpaueb/sec-bert-shape\", num_labels=2)\n","\n","curr_dataset_name = 'shape_short_window_2_labels_car'\n","\n","curr_dataset = load_dataset(f\"thowley824/{curr_dataset_name}\")\n","curr_dataset.set_format(\"torch\")\n","\n","train_dataset = curr_dataset[\"train\"]\n","eval_dataset = curr_dataset[\"test\"]\n","\n","print(f\"\"\"FINE-TUNING DATASET AND MODEL SUMMARY:\n","Model:                  nlpaueb/sec-bert-shape\n","Number of Labels:       2\n","Abnormal Return Metric: CAR\n","Event Window:           1 day before event through 1 day after event\"\"\")\n","\n","notebook_launcher(training_function, (model_7,\n","                                      curr_dataset_name))"],"metadata":{"id":"4YArbrddPKQF"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model_8 = AutoModelForSequenceClassification.from_pretrained(\n","        \"nlpaueb/sec-bert-base\", num_labels=2)\n","\n","curr_dataset_name = 'base_short_window_2_labels_car'\n","\n","curr_dataset = load_dataset(f\"thowley824/{curr_dataset_name}\")\n","curr_dataset.set_format(\"torch\")\n","\n","train_dataset = curr_dataset[\"train\"]\n","eval_dataset = curr_dataset[\"test\"]\n","\n","print(f\"\"\"FINE-TUNING DATASET AND MODEL SUMMARY:\n","Model:                  nlpaueb/sec-bert-base\n","Number of Labels:       2\n","Abnormal Return Metric: CAR\n","Event Window:           1 day before event through 1 day after event\"\"\")\n","\n","notebook_launcher(training_function, (model_8,\n","                                      curr_dataset_name))"],"metadata":{"id":"N0tjIOu4PKWE"},"execution_count":null,"outputs":[]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"A100","machine_shape":"hm","provenance":[],"authorship_tag":"ABX9TyPXlfrXiowrDPUhudtgvCDT"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}